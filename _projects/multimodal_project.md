---
layout: page
title: Visual Story Generation
description: Course Project for 11777 Multimodal Machine Learning
img: assets/img/multimodal.png
importance: 
category: course project
related_publications: false
---

### [[Full Writeup]](/assets/pdf/11_777_Final_Report.pdf)
Partners: [Nevan Giuliani](https://www.linkedin.com/in/nevan-giuliani-0505571a8/), [Alex Lyons](https://www.linkedin.com/in/alexander-lyons-101393255/), [Hyunwoo (Shawn) Park](https://www.linkedin.com/in/shawn-park-5a4644221/)

## Idea
Given an initial text prompt and image, can we generate a story with new story prompts and images? Ideally, the story prompts are not just descriptions of each image but a connecting story. Also, the images should all relate to each other. 
## Datasets
We use the [Visual Storytelling dataset](https://arxiv.org/pdf/1604.03968) for training/evaluation. 


## Implementation Details
Previous work like [IntelligentGrimm](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Intelligent_Grimm_-_Open-ended_Visual_Storytelling_via_Latent_Diffusion_Models_CVPR_2024_paper.html) takes in a full set of story prompts, and generates a set of images using a diffusion model, where each image is conditioned on the previous images. We improve in two ways
- we predict captions along with image so that only an initial caption is required
- we separate the task of predicting story captions and descriptive captions to be entered into the diffusion model, so that the story captions do not have to describe each subsequent image in detail
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="/assets/img/multimodal_figure.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    model diagram. We finetune a multimodal <a href="https://arxiv.org/abs/2304.08485">llava</a> model to predict a descriptive text caption and a story text caption separatly, conditioned on previous images and previous captions. We finetune a diffusion model to predict the next frame in the story, conditioned on previous image and previous captions.  
</div>

We finetune on images, descriptive captions, and story captions from the [Visual Storytelling dataset](https://arxiv.org/pdf/1604.03968). 

## Results

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="/assets/img/multimodal_baselines.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Baselines + Our method. Notice how baselines that do not condition on previous images do not have image to image consistency, and the <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Intelligent_Grimm_-_Open-ended_Visual_Storytelling_via_Latent_Diffusion_Models_CVPR_2024_paper.html">IntelligentGrimm</a> model fails at realism(as it was trained on more cartoon-ish data)
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="/assets/img/multimodal_stories.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Some final stories and descriptive text generated by our pipeline
</div>

